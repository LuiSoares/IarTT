import numpy as np
import random


class Agent:
    def __init__(self,i=0,j=0):
        self.i=i
        self.j=j
    @property
    def loc(self):
        return (self.i,self.j)
    
    def vmove(self, direction):
        direction = 1 if direction > 0 else -1
        return Agent(self.i+direction,self.j)
    
    def hmove(self,direction):
        direction=1 if direction>0 else -1
        return  Agent(self.i,self.j+direction)
    
    def __repr__(self):
        return str(self.loc)

class QLearning:
    def __init__(self,num_states,num_actions,lr=0.1, discount_factor = 1.0):
        self.q = np.zeros((num_states,num_actions))
        self.a = lr
        self.g = discount_factor
    
    def update(self, st , at, rt , st1):
        q = self.q
        a = self.a
        g = self.g
        self.q[st,at] = (1-a)*q[st,at] + a *(rt+g*np.max(q[st1]))
    
        
    
class Maze:
    def __init__(self,rows=4,columns=4):
        self.env=np.zeros((rows,columns))
        self.agent = Agent(0,0)
       
    @property
    def all_actions(self):
        a = self.agent
        return [
            a.vmove(1),
            a.vmove(-1),
            a.hmove(1),
            a.hmove(-1),
        ]
    
    def reset(self):
        self.agent.i=0
        self.agent.j=0
        
        
    def state_for_agent(self,a):
        nr,nc = self.env.shape
        return a.i * nc + a.j
        
    def in_bounds(self,i,j):
        nr,nc = self.env.shape
        return i >= 0 and i < nr and j >= 0 and j < nc
    
    def agent_in_bounds(self,a):
        return self.in_bounds(a.i,a.j)
    
    def agent_dient(self,a):
        return not self.env[a.i,a.j] == -1
    
    def is_valid_new_agent(self,a):
        return self.in_bounds(a.i,a.j) and self.agent_dient(a)
        
    def compute_possible_move(self):
        moves =  self.all_actions
        return [(m,ii) for ii,m in enumerate(moves) if self.is_valid_new_agent(m)]
    
  
        
    def do_a_move(self,a):
        assert self.is_valid_new_agent(a), "Agent canÂ´t go there"
        self.agent = a
        return 10 if self.has_won() else -0.1
        
    def has_won(self):
        a=self.agent
        return self.env[a.i,a.j] == 1
    
    def visualize(self):
        assert self.agent_in_bounds(self.agent), "Agent out of bounds"
        e = self.env.copy()
        m = self.agent
        e[m.i,m.j]=6
        print(e)
        

def make_test_maze(s=4):
    m=Maze(s,s)
    e= m.env
    h,w = e.shape
    e[-1,-1]=1
    for i in range(len(e)):
        for j in range(len(e[i])):
            if i in [0,h-1] and j in [0, w-1]:
                continue
            if random.randint(0,10) > 3:
                e[i,j] = -1
    return m

def main():
    s=5
    q = QLearning(s**2,4)
    m = make_test_maze(s)
    ctu = input()
    if ctu == 'n':
        return
    for i in range(100):
        final_score = 0
        m.reset()
        while not m.has_won():
            moves = m.compute_possible_move()
            random.shuffle(moves)
            move,move_idx = moves[0]
            at = move_idx
            st = m.state_for_agent(m.agent)
            score = m.do_a_move(move)
            final_score +=score
            rt = score
            st1 = m.state_for_agent(m.agent)
            q.update(st,at,rt,st1)
        #print(q.q)
        #print(f"final episode with score of {final_score}")
    m = m.reset()
    while not  m.has_won():
        s = m.state_for_agent(m.agent)
        a_idx = np.argmax(q.q[s])
        m.do_a_move(m.all_actions[a_idx])
        m.visualize()
        
    

if __name__== '__main__':
    main()
    #https://www.youtube.com/watch?v=psDlXfbe6ok